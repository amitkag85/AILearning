{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitkag85/AILearning/blob/master/10_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFsILtyDJiKA"
      },
      "source": [
        "# Semantic Search\n",
        "The core concept of [semantic search](https://en.wikipedia.org/wiki/Semantic_search) is that you can search by **meaning** rather than exact string matching. This has many advantages, such as the ability to find items that mean the same thing but are different on the surface level (e.g. synonyms).\n",
        "\n",
        "The building blocks for this are:\n",
        "*   prepping data (e.g. splitting a document into sub-strings)\n",
        "*   a vectorizer (something that turns input into a vector)\n",
        "*   a (efficient) nearest neighbor algorithm\n",
        "*   a way to store and retrieve metadata (e.g. a `dict`, relational database, etc.)\n",
        "\n",
        "There are two main operations we need to cover, namely indexing (writing) and querying (reading). Of course you could optionally handle updating and deleting as well, but we will ignore that for now.\n",
        "\n",
        "It should be noted that this can work with **any** type of data that you can turn into a vector representation (images, audio, etc.) but for this example we will be working with text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iewMlconQ68K"
      },
      "source": [
        "## Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqxMzqKOFFk8"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4 faiss-cpu InstructorEmbedding lxml nltk numpy sentence-transformers==2.2.2 torch tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BpFRxTUIl4V"
      },
      "source": [
        "## Splitting Text Documents\n",
        "When working with text, you often do not want to treat an entire document as a single entity. This can be for practical reasons, such as it being too long to use as an input to your vectorizer, or for design reasons, such as wanting to search for locations within a document.\n",
        "\n",
        "Here we will be working with sentences within a document. In order to split a string into sentences, we can use an `nltk` tokenizer called `punkt`. Under the hood this looks for periods, newlines, and other subsequences that generally denote the end of a sentence. It is a set of hand written rules and is not perfect, but it will get the job done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTSdq7DcIvhR"
      },
      "outputs": [],
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "text = '''\n",
        "Punkt knows that the periods in Mr. Smith and Johann S. Bach\n",
        "do not mark sentence boundaries.  And sometimes sentences\n",
        "can start with non-capitalized words.  i is a good variable\n",
        "name.\n",
        "'''\n",
        "for i, s in enumerate(sent_detector.tokenize(text.strip())):\n",
        "    print(i, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCuKhS27Iv0y"
      },
      "source": [
        "## Vectorizing Text\n",
        "This is the true secret sauce of semantic search. All of the meaning that we will ultimately be trying to search by must be encoded into a vector *somehow*. This is typically done with a ML model, more specifically a deep neural network, which is trained on some notion of **similarity** between inputs. Since the model's definition of similarity completely determins how results will be calculated, choosing a model that is trained on the domain of your task is critical.\n",
        "\n",
        "**NOTE:** Often the term `embedding` is used interchangeably with vectorizing. They effectively mean the same thing.\n",
        "\n",
        "The [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) family of models is typically used for vectorising text. Specifically when using sentences as inputs, a sentence-transformer is used, which is just a specific way of formatting the output from a BERT model.\n",
        "\n",
        "A recent paper introduced the [Instructor](https://github.com/HKUNLP/instructor-embedding) model, which takes instructions which define the domain, input type, and task to calculate similarity, along with inputs. It was trained on a wide variety of (English) domains and embedding tasks, and is currently one of the best performing options available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aqUtyQAF_2I"
      },
      "outputs": [],
      "source": [
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = INSTRUCTOR('hkunlp/instructor-large')\n",
        "sentences_a = [['Represent the Science sentence: ',\n",
        "                'Parton energy loss in QCD matter'],\n",
        "               ['Represent the Financial statement: ',\n",
        "                'The Federal Reserve on Wednesday raised its benchmark interest rate.']]\n",
        "\n",
        "sentences_b = [['Represent the Science sentence: ',\n",
        "                'The Chiral Phase Transition in Dissipative Dynamics'],\n",
        "               ['Represent the Financial statement: ',\n",
        "                'The funds rose less than 0.5 per cent on Friday']]\n",
        "\n",
        "embeddings_a = vectorizer.encode(sentences_a)\n",
        "embeddings_b = vectorizer.encode(sentences_b)\n",
        "similarities = cosine_similarity(embeddings_a,embeddings_b)\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPYZE4UmHWHl"
      },
      "source": [
        "[FAISS](https://github.com/facebookresearch/faiss/wiki) (Facebook AI Similarity Search) is an efficient [approximate nearest neighbors](https://ignite.apache.org/docs/latest/machine-learning/binary-classification/ann#:~:text=An%20approximate%20nearest%20neighbor%20search,good%20as%20the%20exact%20one.) library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_a.shape"
      ],
      "metadata": {
        "id": "qvIpVDuFcrSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qo1aIheGn2u"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "index = faiss.index_factory(768, 'Flat', faiss.METRIC_INNER_PRODUCT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFZmr5McZeBu"
      },
      "source": [
        "## Speeding Things Up\n",
        "You may have noticed that running our vectorizer earlier took a long time. Instructor is a large [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) and thus does a **LOT** of computation. This can be sped up by using a GPU if you have one available. Fortunately, Google Colab offers GPU runtimes (go to the Runtime menu, select Change Runtime Type, select GPU from the list, and click Save).\n",
        "\n",
        "**NOTE:** Changing the runtime type will restart the machine, so you will need to run everything again.\n",
        "\n",
        "The Instructor model that we are using was implemented using [PyTorch](https://pytorch.org/), so we can use the following code to put our model on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdjqVX0sZeNV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# use the GPU (cuda) if we can, otherwise use CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# put the model on the GPU\n",
        "vectorizer = vectorizer.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_oFrnyXbNP"
      },
      "source": [
        "## Toy Data\n",
        "\n",
        "We will be using the transcript of the [Quanex Building Products Corporation (NX) Q2 2023 Earnings Call](https://news.alphastreet.com/quanex-building-products-corporation-nx-q2-2023-earnings-call-transcript/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb7g6Q0ZXbWW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# the URL of the transcript\n",
        "url = 'https://news.alphastreet.com/quanex-building-products-corporation-nx-q2-2023-earnings-call-transcript/'\n",
        "response = requests.get(url)\n",
        "\n",
        "# parse the HTML so that we can work with it easily\n",
        "soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "sentences = []\n",
        "\n",
        "# the transcript itself is inside this particular div\n",
        "for tag in soup.find_all('div', {'class': 'highlighter-content'}):\n",
        "    # split each block into individual sentences\n",
        "    sentences.extend(sent_detector.tokenize(tag.text.strip()))\n",
        "\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:10]"
      ],
      "metadata": {
        "id": "weqSCVkSd7fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_YFTImVbTrD"
      },
      "source": [
        "## Batching\n",
        "When using deep learning models, you generally want to batch your inputs. This means passing multiple inputs to the model at the same time, which allows for us to use as much parallel compute power as we have. This helper function allows turning a list of inputs into a list of lists of at most size n.\n",
        "\n",
        "**NOTE:** The choice of batch size (n in this function) depends on how much memory your device has. You just need to try things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW1mF9aLde-D"
      },
      "outputs": [],
      "source": [
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXEYrVIdfRP"
      },
      "source": [
        "## Vectorizing\n",
        "Now we will process each sentence from our dataset into vectors. For Instructor, this requires pairing each sentence with an instruction string telling the model how to determine similarity. The steps are:\n",
        "\n",
        "*   pair each sentence with the instruction\n",
        "*   create batches of size `batch_size` to pass to the model\n",
        "*   pass each batch to the model and collect the vectors in a list\n",
        "*   turn the list of vectors into a 2D numpy array that FAISS expects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KspEKOmcbT4x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm  # for a progress bar\n",
        "\n",
        "instruction = 'Represent the financial sentence for information retrieval: '\n",
        "\n",
        "batch_size = 40\n",
        "batches = list(chunks([[instruction, sentence] for sentence in sentences],\n",
        "                      batch_size))\n",
        "\n",
        "vectors = []\n",
        "with torch.no_grad():\n",
        "  for batch in tqdm(batches):\n",
        "    vectors.extend(vectorizer.encode(batch))\n",
        "\n",
        "# make our list of vectors into a 2D array that FAISS needs\n",
        "vectors = np.array(vectors)\n",
        "print(vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha1BXg2rg4EV"
      },
      "source": [
        "## Indexing\n",
        "Here we add our vectors to the FAISS index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdPUTCMTgl41"
      },
      "outputs": [],
      "source": [
        "index.add(vectors)\n",
        "print(index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHwXIxJzg5lk"
      },
      "source": [
        "## Searching\n",
        "Now we can finally search through our data. To do this, we need to convert our query into a vector just like we did when we added our dataset to the index. Then we search the index, asking for `k` results.\n",
        "\n",
        "Searching a FAISS index returns two things: the distances of each result to the query, and the ID of each result. Both of these are 2D lists. The outer list exists because you query FAISS with a **batch** of queries (even if you only have one query, it is wrapped in a list). So the resulting lists are always in the shape `(num_queries, num_results)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPnSp_S3g5xf"
      },
      "outputs": [],
      "source": [
        "# a string to search for\n",
        "query = 'statements that represent opportunities for the business'\n",
        "query_vector = vectorizer.encode([[instruction, query]])\n",
        "k = 5  # how many results to return\n",
        "\n",
        "# D is the distance (in this case cosine similarity) of each result\n",
        "# I is the ID of each result\n",
        "D, I = index.search(query_vector, k)\n",
        "print('matched IDs', I)\n",
        "\n",
        "# the ID of the match is based on the order the vectors were inserted\n",
        "for i, sentence_id in enumerate(I[0]):\n",
        "  print(sentences[sentence_id], D[0][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10_EUN5tCM3c"
      },
      "source": [
        "## Saving and Loading\n",
        "\n",
        "Reading and writing the index from disk is straightforward. As-is this code will save the index to a location that will disappear once the runtime is disconnected. To save it to your google drive, click the `Mount Drive` button and choose a path to save it to.\n",
        "\n",
        "**NOTE:** Keep in mind that saving the index alone does not save the metadata (in our case, the sentences list). You would need to handle that separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdOFK2l0CNEd"
      },
      "outputs": [],
      "source": [
        "faiss.write_index(index, 'transcript.index')\n",
        "index = faiss.read_index('transcript.index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38dleSNVoIzi"
      },
      "source": [
        "# Going Farther\n",
        "This is the bare minimum you need in order to get started with semantic search. However this only scratches the surface of what needs to happen to go beyond a toy example.\n",
        "\n",
        "## Index Types\n",
        "Here we used the `\"Flat\"` FAISS index type, which is exhaustive k-nearest neighbor search. While this is the simplest type, it has the worst performance and is only viable for up to tens of thousands of vectors. There are many options for other more efficient index types depending on needs. [Here](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index) is a good guide for deciding how to configure FAISS.\n",
        "\n",
        "## Metadata\n",
        "In this example, we used an in-memory list of strings to store out sentences, and the ID returned by FAISS corresponded to the position in that list of the associated sentence. If your application requires more than simple lookups, if your data is too large to fit into memory all at once, etc. you likely need something like a relational database. The main consideration is that FAISS IDs can only be integers, so you need a way to coordinate integer IDs between the index and database. There are index types that allow you to provide explicit IDs when adding vectors to FAISS, so that you are not restricted to the order you add vectors determining the ID.\n",
        "\n",
        "## Batteries Included Options\n",
        "Here we intentionally focused on the index aspect of semantic search in order to get a better understanding of the key components it uses. However in practice it can often be preferable to use a service that handles most or all of these pieces directly. Some options are:\n",
        "\n",
        "*   [Milvus](https://milvus.io/) is an open source vector index + database. It still requires you to handle vectorization.\n",
        "*   [Pinecone](https://www.pinecone.io/) is a closed source managed cloud based vector index + database. Again, you need to handle vectorization.\n",
        "*   [Weaviate](https://weaviate.io/) is an open source vector index + database that also offers support for vectorization and has a managed cloud option.\n",
        "*   [Huggingface](https://huggingface.co/inference-endpoints) offers inference endpoints to a number of models, which you could use for vectorization.\n",
        "*   [OpenAI](https://platform.openai.com/docs/guides/embeddings) has an embedding model that you can use via their API.\n",
        "\n",
        "Every option has its own strengths, weaknesses, and considerations. As a rule of thumb, for prototyping or small local projects, the setup we used here is fine. If you need something that will be available over the internet, you can still host what we have, but scaling and maintaining it can quickly become a major undertaking. In those situations you are probaly better off using a more robust option unless you have a good reason not to.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}